# -*- coding: utf-8 -*-
"""klasifikacija.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13sG2RSv2AO8I13z42aeCT08ONQ5gtHuN

First, we will read csv file into pandas data frame
"""

from google.colab import drive
drive.mount('/content/gdrive')

import pandas as pd

df = pd.read_csv('/content/gdrive/My Drive/combined_data.csv')
print(df.shape)

"""Here, we will just split the data into train and test sets. Test set size is 30% of the data, while train set size is 70% (obviously)."""

y = df['class']

# rest of the data
X = df.loc[:, df.columns != 'class']

# split the data into training and test sets
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

"""# CLASSIFICATION

Okay, let's start! We begin wilh KNN classifiers. We will run both weighted (distance) and non-weighted knn for different number of neighbors and see how will they behave.
"""

from sklearn.neighbors import KNeighborsClassifier
import numpy as np
import gc

error = []
ks = [3, 5, 7, 10, 15, 20, 50]

for i in ks:
    knn_i = KNeighborsClassifier(n_neighbors=i)
    knn_i.fit(X_train, y_train)
    
    pred_i = knn_i.predict(X_test)
    error.append(np.abs(np.mean(pred_i != y_test)))
    
    gc.collect()

import matplotlib.pyplot as plt

plt.figure(figsize=(15, 8))  
plt.plot(ks, error, color='red', linestyle='dashed', marker='o',  
         markerfacecolor='blue', markersize=10)

plt.xticks(ks)
plt.title('Error Rate K Value')
plt.xlabel('K Value')
plt.ylabel('Mean Absolute Error')

from sklearn.neighbors import KNeighborsClassifier
import numpy as np
import gc

error_weighted = []
ks = [3, 5, 7, 10, 15, 20, 50]

for i in ks:
    knn_i = KNeighborsClassifier(n_neighbors=i, weights='distance')
    knn_i.fit(X_train, y_train)
    
    pred_i = knn_i.predict(X_test)
    error_weighted.append(np.abs(np.mean(pred_i != y_test)))
    
    gc.collect()

import matplotlib.pyplot as plt

plt.figure(figsize=(15, 8))  
plt.plot(ks, error_weighted, color='red', linestyle='dashed', marker='o',  
         markerfacecolor='blue', markersize=10)

plt.xticks(ks)
plt.title('Error Rate K Value with Weights')  
plt.xlabel('K Value')  
plt.ylabel('Mean Absolute Error')

"""We can see that **KNN with 7 neighbors** is giving us the best result. Also, weighted method might be *a little* bit more accurate. Let us check that by building the following models:"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, classification_report
import numpy as np

n = 7

print('KNN, n = 7, no weights:')

knn = KNeighborsClassifier(n_neighbors=n)
knn.fit(X_train, y_train)

# test data...
y_pred = knn.predict(X_test)

print('Test set acc: {}'.format(knn.score(X_test, y_test)))
print('MAE: {}'.format(np.abs(np.mean(y_pred != y_test)))) # mean absolute error

print()
print('confusion matrix: ')
print(confusion_matrix(y_test, y_pred))
print()
print('classification report:')
print(classification_report(y_test, y_pred))

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, classification_report
import numpy as np

n = 7

print('KNN, n = 7, weights = distance:')

knn = KNeighborsClassifier(n_neighbors=n, weights='distance')
knn.fit(X_train, y_train)

# test data...
y_pred = knn.predict(X_test)

print('Test set acc: {}'.format(knn.score(X_test, y_test)))
print('MAE: {}'.format(np.abs(np.mean(y_pred != y_test)))) # mean absolute error

print()
print('confusion matrix: ')
print(confusion_matrix(y_test, y_pred))
print()
print('classification report:')
print(classification_report(y_test, y_pred))

"""So, the weighed model is better, but the difference is negligible. These models are doing very poorly in classification of class2, the class with the smallest number of patterns.

We continue with **Naive Bayes** classificators. NB classificators won't probably have very good results, because of the data, but we will see.
"""

from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import ComplementNB

mnb = MultinomialNB()
mnb.fit(X_train, y_train)
print('MultinomialNB train acc: {}'.format(mnb.score(X_train, y_train)))
print('MultinomialNB test acc: {}'.format(mnb.score(X_test, y_test)))


cnb = ComplementNB()
cnb.fit(X_train, y_train)
print('ComplementNB train acc: {}'.format(cnb.score(X_train, y_train)))
print('ComplementNB test acc: {}'.format(cnb.score(X_test, y_test)))

"""We can clearly see that the KNN model is better.
The next model will be **SVM**. Without any particular reason. classification will start with **rbf** kernel, and than we will try the others.
"""

from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix
import numpy as np

clf = SVC(C=100, kernel='rbf', gamma=0.1)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

print('MAE: {}'.format(np.abs(np.mean(y_pred != y_test))))
print(confusion_matrix(y_test, y_pred))

"""The results we got are bad. As we can see, this classifier is telling us that every class is class5.
We will now check will polynomial kernel behave better, and for which degree.
"""

from sklearn.svm import SVC
import numpy as np

error = []
degrees = [1, 2, 3, 4, 5, 6]

for d in degrees:
    clf = SVC(C=100, kernel='poly', degree=d, gamma=0.1)
    clf.fit(X_train, y_train)
    
    y_pred = clf.predict(X_test)
    error.append(np.abs(np.mean(y_pred != y_test)))

import matplotlib.pyplot as plt

plt.figure(figsize=(15, 8))  
plt.plot(degrees, error, color='red', linestyle='dashed', marker='o',  
         markerfacecolor='blue', markersize=10)

plt.xticks(degrees)
plt.title('Error Rate for different degree values in SVM with polynomial kernel (gamma is 0.1)')  
plt.xlabel('Degree values')  
plt.ylabel('Mean Absolute Error')

"""So, here we have really good results. For polynomial kernel, where d = 1, we have the best results so far, MAE appears to be just 0.06, **awesome!**

Now let's run SVM again and see how it behaves for different value of gamma. Here, we set gamma to 'scale'.
"""

from sklearn.svm import SVC
import numpy as np

error = []
degrees = [1, 2, 3, 4, 5, 6]

for d in degrees:
    clf = SVC(C=100, kernel='poly', degree=d, gamma='scale')
    clf.fit(X_train, y_train)
    
    y_pred = clf.predict(X_test)
    error.append(np.abs(np.mean(y_pred != y_test)))

import matplotlib.pyplot as plt

plt.figure(figsize=(15, 8))  
plt.plot(degrees, error, color='red', linestyle='dashed', marker='o',  
         markerfacecolor='blue', markersize=10)

plt.xticks(degrees)
plt.title('Error Rate for different degree values in SVM with polynomial kernel (gamma = \'scale\')')  
plt.xlabel('Degree values')  
plt.ylabel('Mean Absolute Error')

"""**Okay, THIS IS EPIC!**


Let's test the model where d = 1 and gamma = 'scale', what appears to be our best model so far.
"""

from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, classification_report

clf = SVC(C=100, kernel='poly', degree=1, gamma='scale')
clf.fit(X_train, y_train)

print('Train set acc: {}'.format(clf.score(X_train, y_train)))
print('Test set acc: {}'.format(clf.score(X_test, y_test)))

y_pred = clf.predict(X_test)

print('confusion matrix:')
print(confusion_matrix(y_test, y_pred))
print()
print('classification report:')
print(classification_report(y_test, y_pred))

"""**It is!** The classification accuracy between train and test sets exist, but is not significant, I think. Now we will increase C parameter."""

from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, classification_report

clf = SVC(C=300, kernel='poly', degree=1, gamma='scale')
clf.fit(X_train, y_train)

print('Train set acc: {}'.format(clf.score(X_train, y_train)))
print('Test set acc: {}'.format(clf.score(X_test, y_test)))

y_pred = clf.predict(X_test)

print('confusion matrix:')
print(confusion_matrix(y_test, y_pred))
print()
print('classification report:')
print(classification_report(y_test, y_pred))

"""The result is better on train set, but slightly worse on test set. The biggest difference in accuracy can be observed in classification of class2, where smaller margin has the edge.

I would say that the model with lower value for C is better, slightly, but better.

Okay, the next in line are the ensemble methods, and we will use them to try to improve the results we got so far.
A lot of differents models will be tested.

We will first start with bagging methods, primarily with **Random Forest Classifier**.
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix

rfc = RandomForestClassifier(n_estimators=100, criterion='gini')
rfc.fit(X_train, y_train)

print('Train set acc: {}'.format(rfc.score(X_train, y_train)))
print('Test set acc: {}'.format(rfc.score(X_test, y_test)))

y_pred = rfc.predict(X_test)
print('confusion matrix:')
print(confusion_matrix(y_test, y_pred))

"""I tried different number of estimators for gini index and always got roughly the same result, accuracy on a test set is around 0.85, which is not bad, but it is really not very good either.

Now we will check if model with entropy as a criterion act differently.
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix

rfc = RandomForestClassifier(n_estimators=100, criterion='entropy')
rfc.fit(X_train, y_train)

print('Train set acc: {}'.format(rfc.score(X_train, y_train)))
print('Test set acc: {}'.format(rfc.score(X_test, y_test)))

y_pred = rfc.predict(X_test)
print('confusion matrix:')
print(confusion_matrix(y_test, y_pred))

"""The results are similar, accuracy on a test set is, again, for different number of estimators, around 0.85, although the results with entropy are little worse for lower estimator numbers.

Now we will try to upgrade our accuracy with bagging on some models that have already gave us good results, **KNN** with seven neighbors and **SVM** with polynomial kernel with one degree. 

We got pretty much the same results for weighted and non-weightd method in KNN before, so our model now won't use weights, without any particular reason.
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import BaggingClassifier

knn = KNeighborsClassifier(n_neighbors=7)

bclf = BaggingClassifier(base_estimator=knn, n_estimators=3, max_samples=0.4)
bclf.fit(X_train, y_train)

print('Train set acc: {}'.format(bclf.score(X_train, y_train)))
print('Test set acc: {}'.format(bclf.score(X_test, y_test)))

"""Prameters are, n_estimators = 3 and max_samples = 0.4. Unfortunately,  for greater values I got MemoryError, A.K.A. not enough RAM, so if I want to increase one of them, I also have do decrease the other. This means that our flexibility very is limited.

The result is worse than it was without bagging, but not by much, and might be better for larger values of the parameters, but, as mentioned, we can't test that.

Now we will do the same with SVM.
"""

from sklearn.svm import SVC
from sklearn.ensemble import BaggingClassifier

svm = SVC(C=100, kernel='poly', degree=1, gamma='scale')

bclf = BaggingClassifier(base_estimator=svm, n_estimators=20, max_samples=0.5)
bclf.fit(X_train, y_train)

print('Train set acc: {}'.format(bclf.score(X_train, y_train)))
print('Test set acc: {}'.format(bclf.score(X_test, y_test)))

from sklearn.svm import SVC
from sklearn.ensemble import BaggingClassifier

svm = SVC(C=100, kernel='poly', degree=1, gamma='scale')

bclf = BaggingClassifier(base_estimator=svm, n_estimators=20, max_samples=0.6)
bclf.fit(X_train, y_train)

# print('Train set acc: {}'.format(bclf.score(X_train, y_train))) # this takes a lot of time so I commented it :D
print('Test set acc: {}'.format(bclf.score(X_test, y_test)))

"""The hardware limitations are still present, but for this model we could put greater values for parameters then in the knn one.

The results are, unfortunately, worse than the non-bagging SVM, but not by much, and the difference is almost insignificant. The accuracy in bagging method could potentially increase for larger values for parameters, but we can't test it. We move onto the next models.

Okay, with bagging finished, we can focus on boosting. We will use **AdaBoost** for our classification.
Because knn does not support sample weights, we start with svm.

I should mention that svm does not support the calculation of class probabilities with a predict_proba method, so we cannot use SAMME.R algorithm. Instead, we use SAMME, which takes more iterations, and usually gives us worse accuracy on test sets (from sckit-learn.org). Maybe that is the reason program always crashes. Will have to look that later **again**.
"""

from sklearn.svm import SVC
from sklearn.ensemble import AdaBoostClassifier

svm = SVC(C=100, kernel='poly', degree=1, gamma='scale')

abclf = AdaBoostClassifier(base_estimator=svm, n_estimators=3, algorithm='SAMME')
abclf.fit(X_train, y_train)

# print('Train set acc: {}'.format(abclf.score(X_train, y_train)))
print('Test set acc: {}'.format(abclf.score(X_test, y_test)))

"""Because we can't boost neither knn nor, apparently, svm, we will boost some other methods, and observe the results. We begin with Naive Bayes."""

from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import AdaBoostClassifier

mnb = MultinomialNB()

abclf = AdaBoostClassifier(base_estimator=mnb, n_estimators=100)
abclf.fit(X_train, y_train)

print('MNB Train set acc: {}'.format(abclf.score(X_train, y_train)))
print('MNB Test set acc: {}'.format(abclf.score(X_test, y_test)))

from sklearn.naive_bayes import ComplementNB
from sklearn.ensemble import AdaBoostClassifier

cnb = ComplementNB()

for i in [50, 100, 200]:
    abclf = AdaBoostClassifier(base_estimator=cnb, n_estimators=i)
    abclf.fit(X_train, y_train)

    print('n_estimators={}'.format(i))
    print('CNB Train set acc: {}'.format(abclf.score(X_train, y_train)))
    print('CNB Test set acc: {}'.format(abclf.score(X_test, y_test)))

"""The results are bad, but also interesting. That's it, **Next**!

We will now try to improve decision tree classifier with boosting. The result probably won't be better than SVM but it does not hurt to try.
"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier

dtc = DecisionTreeClassifier()

abclf = AdaBoostClassifier(base_estimator=dtc, n_estimators=50)
abclf.fit(X_train, y_train)

print('Train set acc: {}'.format(abclf.score(X_train, y_train)))
print('Test set acc: {}'.format(abclf.score(X_test, y_test)))

"""The results are nothing special, as expected.

Can we improve RandomForest, the bagging algorithm, with boosting? Let's try.
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier

rfc = RandomForestClassifier(n_estimators=100)

abclf = AdaBoostClassifier(base_estimator=rfc, n_estimators=250)
abclf.fit(X_train, y_train)

print('Train set acc: {}'.format(abclf.score(X_train, y_train)))
print('Test set acc: {}'.format(abclf.score(X_test, y_test)))

"""Well, the results *did* improve in random forest model with bagging, but for very little. Maybe we could get further improvements if we increase number of estimators, but I highly doubt that those improvements will be significant because the difference in results for 50 and 250 estimators is almost non-existent.

And now, our last approach, **Voting classifiers**. We will create some models from before, and do the voting.
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC

from sklearn.ensemble import VotingClassifier

rfc = RandomForestClassifier(n_estimators=100, criterion='gini')
knn = KNeighborsClassifier(n_neighbors=7)
mnb = MultinomialNB()
svc = SVC(C=100, kernel='poly', degree=1, gamma='scale')

mdl = VotingClassifier(estimators=[('RFC', rfc), ('KNN', knn), ('MNB', mnb), ('SVM', svc)], voting='hard')
mdl.fit(X_train, y_train)

print('Train set acc: {}'.format(mdl.score(X_train, y_train)))
print('Test set acc: {}'.format(mdl.score(X_test, y_test)))

"""The results are good, but the difference in accuracy on test and training sets is significant. Maybe we will see the improvement in out next model.

The same thing, with same models used for voting, but with weights for each of the models added. Weights have the value they have because the better models from before should have greater weights. Naive Bayes one is bad, but I used it nonetheless.
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC

from sklearn.ensemble import VotingClassifier

rfc = RandomForestClassifier(n_estimators=100, criterion='gini')
knn = KNeighborsClassifier(n_neighbors=7)
mnb = MultinomialNB() 
svc = SVC(C=100, kernel='poly', degree=1, gamma='scale')

mdl = VotingClassifier(estimators=[('RFC', rfc), ('KNN', knn), ('MNB', mnb), ('SVM', svc)], voting='hard', weights=[1.5, 1.7, 1, 2.5])
mdl.fit(X_train, y_train)

print('Train set acc: {}'.format(mdl.score(X_train, y_train)))
print('Test set acc: {}'.format(mdl.score(X_test, y_test)))

"""The results are better in this model than in the previous one, but they are not our best model so far.

Now we will do the voting, but the models used will be just SVC with different parameters.
"""

from sklearn.svm import SVC
from sklearn.ensemble import VotingClassifier

svc1 = SVC(C=100, kernel='poly', degree=1, gamma='scale')
svc2 = SVC(C=100, kernel='poly', degree=3, gamma='scale')
svc3 = SVC(C=100, kernel='poly', degree=1, gamma=0.1)
svc4 = SVC(C=100, kernel='poly', degree=3, gamma=0.1)

mdl = VotingClassifier(estimators=[('SVC1', svc1), ('SVC2', svc2), ('SVC3', svc3), ('SVC4', svc4)], voting='hard', weights=[1.2, 1, 1.2, 1])
mdl.fit(X_train, y_train)

print('Train set acc: {}'.format(mdl.score(X_train, y_train)))
print('Test set acc: {}'.format(mdl.score(X_test, y_test)))

"""Accuracy is high, but we had a slighly better model.

Our last model will be soft voting. We need models with probabilites, so the knn is out. After that, we will test this model on random patterns from the test data.
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC

from sklearn.ensemble import VotingClassifier

rfc = RandomForestClassifier(n_estimators=100, criterion='gini')
dt = DecisionTreeClassifier()
mnb = MultinomialNB()
svc = SVC(C=100, kernel='poly', degree=1, gamma='scale', probability=True)

mdl = VotingClassifier(estimators=[('RFC', rfc), ('DT', dt), ('MNB', mnb), ('SVM', svc)], voting='soft')
mdl.fit(X_train, y_train)

# random petterns from test set
print('pattern1')
print(mdl.predict_proba([X_test.iloc[4444]]))

print('pattern2')
print(mdl.predict_proba([X_test.iloc[800]]))

print('pattern3')
print(mdl.predict_proba([X_test.iloc[300]]))

print('pattern4')
print(mdl.predict_proba([X_test.iloc[2222]]))

print('pattern5')
print(mdl.predict_proba([X_test.iloc[1471]]))

print('pattern6')
print(mdl.predict_proba([X_test.iloc[0]]))

"""This is showing us the probability that the selected pattern belongs to some of the classes.
Pattern1 the most probable belongs to class6. That is probably true because probability is almost 0.96.
The highest probability for pattern2 is 0.67 for class6 also. But it should be noted that the probability for class1 is also high, compared to the others.
If we continue like this, we have that pattern3 -> class1, pattern4 -> class6, pattern5 -> class4, pattern6 -> class3.

## SCALED DATA

Now, out of curiosity, we test or models on data that is scaled to **[0, 1]**. Who knows, maybe the accuracy will imporove, although it probably won't.

Therefore, we normalize data, split it into train(30%) and test(70% duh) sets, and start modeling.
"""

# min max scaler (a-b) -> (0-1)
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

scl = MinMaxScaler()
X = scl.fit_transform(X)

X_train_scaled, X_test_scaled, y_train, y_test = train_test_split(X, y, test_size=0.3)

from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import ComplementNB

mnb = MultinomialNB()
mnb.fit(X_train_scaled, y_train)
print('MultinomialNB train acc: {}'.format(mnb.score(X_train_scaled, y_train)))
print('MultinomialNB test acc: {}'.format(mnb.score(X_test_scaled, y_test)))


cnb = ComplementNB()
cnb.fit(X_train_scaled, y_train)
print('ComplementNB train acc: {}'.format(cnb.score(X_train_scaled, y_train)))
print('ComplementNB test acc: {}'.format(cnb.score(X_test_scaled, y_test)))

"""So, the results we got with the **Naive Bayes** classificators on scaled data are better, especially on MultinomialNB. But, the difference between accuracy on train and test sets is significant. Therefore, we might have overfitting here...



We will now see how **KNN** models behave on a scaled data.
"""

from sklearn.neighbors import KNeighborsClassifier
import numpy as np
import gc

error = []
ks = [5, 7, 15, 25, 50, 75, 100, 125]

for i in ks:
    knn_i = KNeighborsClassifier(n_neighbors=i)
    knn_i.fit(X_train_scaled, y_train)
    
    pred_i = knn_i.predict(X_test_scaled)
    error.append(np.abs(np.mean(pred_i != y_test)))
    
    gc.collect()

import matplotlib.pyplot as plt

plt.figure(figsize=(15, 8))  
plt.plot(ks, error, color='red', linestyle='dashed', marker='o',  
         markerfacecolor='blue', markersize=10)

plt.xticks(ks)
plt.title('Error Rate K Value')
plt.xlabel('K Value')
plt.ylabel('Mean Absolute Error')

"""The results are significantly worse when we scale data. The MAE appears to decrease in every iteration although the decrease is minimal in the end. This model is not good.

The next in line is SVM, rbf and poly kernels.
"""

from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix
import numpy as np

clf = SVC(C=100, kernel='rbf', gamma=0.1)
clf.fit(X_train_scaled, y_train)

y_pred = clf.predict(X_test_scaled)

print('MAE: {}'.format(np.abs(np.mean(y_pred != y_test))))
print(confusion_matrix(y_test, y_pred))

from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix
import numpy as np

clf = SVC(C=1, kernel='rbf', gamma=0.1)
clf.fit(X_train_scaled, y_train)

y_pred = clf.predict(X_test_scaled)

print('MAE: {}'.format(np.abs(np.mean(y_pred != y_test))))
print(confusion_matrix(y_test, y_pred))

"""RGF kernel is indeed more accurate on normalized data but not by much."""

from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix
import numpy as np

clf = SVC(C=100, kernel='poly', degree=3, gamma=0.1)
clf.fit(X_train_scaled, y_train)

y_pred = clf.predict(X_test_scaled)

print('MAE: {}'.format(np.abs(np.mean(y_pred != y_test))))
print(confusion_matrix(y_test, y_pred))

"""Polynomial kernel behaves okayish, but the results on a non-scaled data are better. **Much better!**

Scaling data did nothing good for us, though It did improve Naive Bayes classificators. Also, we had a small improvement in SVM with RBF kernel, but nothing significant.
"""