# -*- coding: utf-8 -*-
"""classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GoGg_k6f7oEHX6V3RNSmhXsy75PoB4Gz
"""

from google.colab import drive
drive.mount('/content/gdrive')

import pandas as pd

df = pd.read_csv('/content/gdrive/My Drive/ip_files/data/data_without_outliers.csv')
print(df.shape)

"""Split on train(70%) and test(30%) sets"""

y = df['class']

# rest of the data
X = df.loc[:, df.columns != 'class']

# split the data into training and test sets
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=27)

"""free some of the much needed memory"""

del X
del y
del df

import gc
gc.collect()

"""# CLASSIFICATION

## KNN


Okay, let's start! We begin wilh **KNN** classifiers. We will run both weighted (distance) and non-weighted knn for different number of neighbors and see how will they behave.

We choose those values because, well, we need to start from somewhere. Also, higher values probably won't do well because of the number of samples of the class2
"""

from sklearn.neighbors import KNeighborsClassifier
import numpy as np
import gc

error = []
ks = [3, 5, 6, 7, 8, 9, 10, 15]

for i in ks:
    knn_i = KNeighborsClassifier(n_neighbors=i)
    knn_i.fit(X_train, y_train)
    
    pred_i = knn_i.predict(X_test)
    error.append(np.abs(np.mean(pred_i != y_test)))
    
    gc.collect()

import matplotlib.pyplot as plt

plt.figure(figsize=(15, 8))  
plt.plot(ks, error, color='red', linestyle='dashed', marker='o',  
         markerfacecolor='blue', markersize=10)

plt.xticks(ks)
plt.title('Error Rate K Value')
plt.xlabel('K Value')
plt.ylabel('Mean Absolute Error')

"""The results shown on the plot are good. There is also not a significant difference in error value for different **k** values. That can better be observed if we plot **y-axis** from 0 to 1. The best result is **k = 6**.

We will try KNN again, but this time we will add weights. Our weighting method is distance.
"""

from sklearn.neighbors import KNeighborsClassifier
import numpy as np
import gc

error_weighted = []
ks = [3, 5, 6, 7, 8, 9, 10, 15]

for i in ks:
    knn_i = KNeighborsClassifier(n_neighbors=i, weights='distance')
    knn_i.fit(X_train, y_train)
    
    pred_i = knn_i.predict(X_test)
    error_weighted.append(np.abs(np.mean(pred_i != y_test)))
    
    gc.collect()

import matplotlib.pyplot as plt

plt.figure(figsize=(15, 8))  
plt.plot(ks, error_weighted, color='red', linestyle='dashed', marker='o',  
         markerfacecolor='blue', markersize=10)

plt.xticks(ks)
plt.title('Error Rate K Value with Weights')  
plt.xlabel('K Value')  
plt.ylabel('Mean Absolute Error')

"""The results between this and non-weighted method are similar, but this one is slightly better.

Let's create what appears to be our best knn model. We will set k value to 6 and we will use distance as weight.
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

knn = KNeighborsClassifier(n_neighbors=6, weights='distance')
knn.fit(X_train, y_train)

y_pred = knn.predict(X_test)

print('accuracy train: {}'.format(np.round(knn.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

"""The results are good, but there is a problem in classifing classes 2 and 3 as we can see.

Now we will save this model into pickle file
"""

from joblib import dump, load
dump(knn, '/content/gdrive/My Drive/ip_files/models/knn_weighted_6neighbors.pkl')

# dtc = load('/content/gdrive/My Drive/ip_files/primer.pkl')

"""## Naive Bayes

We continue with **Naive Bayes** classificators. NB classificators won't probably have very good results, because of the data (numerical), but we will see.

MultinomianNB:
"""

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

mnb = MultinomialNB()
mnb.fit(X_train, y_train)

y_pred = mnb.predict(X_test)

print('accuracy train: {}'.format(np.round(mnb.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

from joblib import dump
dump(mnb, '/content/gdrive/My Drive/ip_files/models/multinomialNB.pkl')

"""This model is particularly bad at classifing class2, but okay overall.

ComplementNB:
"""

from sklearn.naive_bayes import ComplementNB
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

cnb = ComplementNB()
cnb.fit(X_train, y_train)

y_pred = cnb.predict(X_test)

print('accuracy train: {}'.format(np.round(cnb.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

from joblib import dump
dump(cnb, '/content/gdrive/My Drive/ip_files/models/complementNB.pkl')

"""cnb does have a little bit higher accuracy than mnb, but it fails to classify some classes.

The results we got are okay, but KNN gave us better results.

## Decision trees

Now, onto Decision trees. We start with gini as quality of split
"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np


dtc = DecisionTreeClassifier(criterion='gini')
dtc.fit(X_train, y_train)

y_pred = dtc.predict(X_test)

print('accuracy train: {}'.format(np.round(dtc.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

from joblib import dump
dump(dtc, '/content/gdrive/My Drive/ip_files/models/decisionTree_gini.pkl')

"""We will also try entropy as a criterion"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np


dtc = DecisionTreeClassifier(criterion='entropy')
dtc.fit(X_train, y_train)

y_pred = dtc.predict(X_test)

print('accuracy train: {}'.format(np.round(dtc.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

from joblib import dump
dump(dtc, '/content/gdrive/My Drive/ip_files/models/decisionTree_entropy.pkl')

"""The entropy as a criterion is a little bit better, but both models are stuggling to classifiy classes 2 and 3 correctly.

Now, we would like to add class_weights. We will set parameter class_weight in DecisionTreeClassifier to 'balanced' meaning that sklearn will evaluate class weight for every class. We will try this approach with both gini and entropy.

Gini:
"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np


dtc = DecisionTreeClassifier(criterion='gini', class_weight='balanced')
dtc.fit(X_train, y_train)

y_pred = dtc.predict(X_test)

print('accuracy train: {}'.format(np.round(dtc.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

from joblib import dump
dump(dtc, '/content/gdrive/My Drive/ip_files/models/decisionTree_balanced_gini.pkl')

"""Entropy:"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np


dtc = DecisionTreeClassifier(criterion='entropy', class_weight='balanced')
dtc.fit(X_train, y_train)

y_pred = dtc.predict(X_test)

print('accuracy train: {}'.format(np.round(dtc.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

from joblib import dump
dump(dtc, '/content/gdrive/My Drive/ip_files/models/decisionTree_balanced_entropy.pkl')

"""The results have gotten worse. We menaged to classify some instances of the class2 correctly, but with price.

## SVM

The next model will be **SVM**. Classification will start with **rbf** kernel, and then we will try others.

### rbf kernel
"""

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

clf = SVC(C=100, kernel='rbf', gamma='scale')
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

print('accuracy train: {}'.format(np.round(clf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

from joblib import dump
dump(clf, '/content/gdrive/My Drive/ip_files/models/svm_rbf_C100_gamaScale.pkl')

"""Lets increase C parameter."""

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

clf = SVC(C=300, kernel='rbf', gamma='scale')
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

print('accuracy train: {}'.format(np.round(clf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

from joblib import dump
dump(clf, '/content/gdrive/My Drive/ip_files/models/svm_rbf_C300_gamaScale.pkl')

"""This model has very high accuracy. It is also out best model so far in classifying class two (Recall gor class2 is not high though)."""

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np


clf = SVC(C=100, kernel='rbf', gamma='scale', class_weight='balanced')
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

print('accuracy train: {}'.format(np.round(clf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

from joblib import dump
dump(clf, '/content/gdrive/My Drive/ip_files/models/svm_rbf_C100_gamaScale_balancedWeights.pkl')

"""The results are slighly worse when we balance weights

Increase C parameter. That parameter represents the penalty for errors in classification. Lower the C the lower accuracy on train set
"""

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np


clf = SVC(C=300, kernel='rbf', gamma='scale', class_weight='balanced')
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

print('accuracy train: {}'.format(np.round(clf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

from joblib import dump
dump(clf, '/content/gdrive/My Drive/ip_files/models/svm_rbf_C300_gamaScale_balancedWeights.pkl')

"""and now we will decrease C parameter (make margine larger)"""

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np


clf = SVC(C=10, kernel='rbf', gamma='scale', class_weight='balanced')
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

print('accuracy train: {}'.format(np.round(clf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

"""We got very good results here. Every model kinda struggles to classify elements of class2. The one where C = 10 did classify the larger number of patterns of class2, but that model also made a lot of mistakes in that classification (look at recall and f1 score)

### poly kernel

Now, polynomial kernel, gamma value is 'scale'. We will see wich degree will give us the best result
"""

from sklearn.svm import SVC
import numpy as np

error = []
degrees = [1, 2, 3, 4, 5, 6]

for d in degrees:
    clf = SVC(C=100, kernel='poly', degree=d, gamma='scale')
    clf.fit(X_train, y_train)
    
    y_pred = clf.predict(X_test)
    error.append(np.abs(np.mean(y_pred != y_test)))

import matplotlib.pyplot as plt

plt.figure(figsize=(15, 8))  
plt.plot(degrees, error, color='red', linestyle='dashed', marker='o',  
         markerfacecolor='blue', markersize=10)

plt.xticks(degrees)
plt.title('Error Rate for different degree values in SVM with polynomial kernel (gamma is 0.1)')  
plt.xlabel('Degree values')  
plt.ylabel('Mean Absolute Error')

"""The results are very good, but can we do better?

Everything is the same, we will just balance classes
"""

from sklearn.svm import SVC
import numpy as np

error = []
degrees = [1, 2, 3, 4, 5, 6]

for d in degrees:
    clf = SVC(C=100, kernel='poly', degree=d, gamma='scale', class_weight='balanced')
    clf.fit(X_train, y_train)
    
    y_pred = clf.predict(X_test)
    error.append(np.abs(np.mean(y_pred != y_test)))

import matplotlib.pyplot as plt

plt.figure(figsize=(15, 8))  
plt.plot(degrees, error, color='red', linestyle='dashed', marker='o',  
         markerfacecolor='blue', markersize=10)

plt.xticks(degrees)
plt.title('Error Rate for different degree values in SVM with polynomial kernel (gamma is 0.1)')  
plt.xlabel('Degree values')  
plt.ylabel('Mean Absolute Error')

"""Results on non-balanced data are a little bit better. Poly with degree ONE gave us very good results."""

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

clf = SVC(C=100, kernel='poly', gamma='scale', degree=1, class_weight=None)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

print('accuracy train: {}'.format(np.round(clf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

from joblib import dump
dump(clf, '/content/gdrive/My Drive/ip_files/models/svm_poly_C100_gamaScale_degree1.pkl')

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

clf = SVC(C=300, kernel='poly', gamma='scale', degree=1, class_weight=None)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

print('accuracy train: {}'.format(np.round(clf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

from joblib import dump
dump(clf, '/content/gdrive/My Drive/ip_files/models/svm_poly_C300_gamaScale_degree1.pkl')

"""These are good results. Model with larger C value classify class2 better (look at f score and recall), and has higher accuracy on train and test sets.

Let's increase C even more.
"""

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

clf = SVC(C=600, kernel='poly', gamma='scale', degree=1, class_weight=None)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

print('accuracy train: {}'.format(np.round(clf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

"""now balanced, we will change C parameter"""

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

clf = SVC(C=100, kernel='poly', gamma='scale', degree=1, class_weight='balanced')
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

print('accuracy train: {}'.format(np.round(clf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

from joblib import dump
dump(clf, '/content/gdrive/My Drive/ip_files/models/svm_poly_C100_gamaScale_degree1_balancedWeights.pkl')

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

clf = SVC(C=300, kernel='poly', gamma='scale', degree=1, class_weight='balanced')
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

print('accuracy train: {}'.format(np.round(clf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

from joblib import dump
dump(clf, '/content/gdrive/My Drive/ip_files/models/svm_poly_C300_gamaScale_degree1_balancedWeights.pkl')

"""Results with balanced class weights are also good, but I would say that the model with non-balanced class weights and C = 300 is our best model, out of all SVCs with poly kernels.

### Linear kernel
"""

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

clf = SVC(C=100, kernel='linear', gamma='scale')
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

print('accuracy train: {}'.format(np.round(clf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

from joblib import dump
dump(clf, '/content/gdrive/My Drive/ip_files/models/svm_linear_C100_gamaScale.pkl')

"""now let's try balanced class weights"""

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

clf = SVC(C=100, kernel='linear', gamma='scale', class_weight='balanced')
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

print('accuracy train: {}'.format(np.round(clf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

"""Oddly enough, we got the same results for balanced and non-balanced linear svm.

Increase C parameter and try again.
"""

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

clf = SVC(C=300, kernel='linear', gamma='scale', class_weight='balanced')
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

print('accuracy train: {}'.format(np.round(clf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

"""This result is the same as the others. This model is very good but poly and rbf are a little bit better

## Neural networks

Next in line are Neural networks. We will create different models with differents solvers.
First, we create a model with default values.
"""

from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

clf = MLPClassifier()
clf.fit(X_train, y_train)
                    
y_pred = clf.predict(X_test)

print('accuracy train: {}'.format(np.round(clf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

"""The results are okay. No patterns are classified as class2. Class3 is also badly classified.

Now, we will try different solvers. Hidden layers will also be changed. Let's add that we are expecting the best results with adam solver because that solver usually gives as good results on larga datasets.
"""

from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

n = len(X_train.loc[0, :])
clf = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(n // 16, n // 64), activation='relu')
clf.fit(X_train, y_train)
                    
y_pred = clf.predict(X_test)

print('accuracy train: {}'.format(np.round(clf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

"""There is **not enough RAM to run this block of code**, so we will continue with other solvers."""

from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

n = len(X_train.loc[0, :])
clf = MLPClassifier(solver='sgd', hidden_layer_sizes=(n // 16, n // 64), activation='relu')
clf.fit(X_train, y_train)
                    
y_pred = clf.predict(X_test)

print('accuracy train: {}'.format(np.round(clf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

n = len(X_train.loc[0, :])
clf = MLPClassifier(solver='adam', hidden_layer_sizes=(n // 16, n // 64), activation='relu')
clf.fit(X_train, y_train)
                    
y_pred = clf.predict(X_test)

print('accuracy train: {}'.format(np.round(clf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

from joblib import dump
dump(clf, '/content/gdrive/My Drive/ip_files/models/neuralNetwork_solverAdam_activationRelu_layers_n16_n64.pkl')

"""Now we will create model with the best solver, but we will change activation function to tanh and then to logistic."""

from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

n = len(X_train.loc[0, :])
clf = MLPClassifier(solver='adam', activation='tanh', hidden_layer_sizes=(n // 16, n // 64))
clf.fit(X_train, y_train)
                    
y_pred = clf.predict(X_test)

print('accuracy train: {}'.format(np.round(clf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

n = len(X_train.loc[0, :])
clf = MLPClassifier(solver='adam', activation='logistic', hidden_layer_sizes=(n // 16, n // 64))
clf.fit(X_train, y_train)
                    
y_pred = clf.predict(X_test)

print('accuracy train: {}'.format(np.round(clf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

"""No improvments, these models are worse than the adam one.

Let's play with hidden layers now.
"""

from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

n = len(X_train.loc[0, :])
clf = MLPClassifier(solver='adam', activation='relu', hidden_layer_sizes=(n // 64))
clf.fit(X_train, y_train)
                    
y_pred = clf.predict(X_test)

print('accuracy train: {}'.format(np.round(clf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

n = len(X_train.loc[0, :])
clf = MLPClassifier(solver='adam', activation='relu', hidden_layer_sizes=(n // 32))
clf.fit(X_train, y_train)
                    
y_pred = clf.predict(X_test)

print('accuracy train: {}'.format(np.round(clf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

n = len(X_train.loc[0, :])
clf = MLPClassifier(solver='adam', activation='relu', hidden_layer_sizes=(n // 32, n // 64))
clf.fit(X_train, y_train)
                    
y_pred = clf.predict(X_test)

print('accuracy train: {}'.format(np.round(clf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

n = len(X_train.loc[0, :])
clf = MLPClassifier(solver='adam', activation='relu', hidden_layer_sizes=(n // 16, n // 64, n // 128))
clf.fit(X_train, y_train)
                    
y_pred = clf.predict(X_test)

print('accuracy train: {}'.format(np.round(clf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

from joblib import dump
dump(clf, '/content/gdrive/My Drive/ip_files/models/neuralNetwork_solverAdam_activationRelu_layers_n16_n64_n128.pkl')

from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

n = len(X_train.loc[0, :])
clf = MLPClassifier(solver='adam', activation='relu', hidden_layer_sizes=(n // 32, n // 64, n // 128))
clf.fit(X_train, y_train)
                    
y_pred = clf.predict(X_test)

print('accuracy train: {}'.format(np.round(clf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

n = len(X_train.loc[0, :])
clf = MLPClassifier(solver='adam', activation='relu', hidden_layer_sizes=(n // 8, n // 32, n // 64, n // 128))
clf.fit(X_train, y_train)
                    
y_pred = clf.predict(X_test)

print('accuracy train: {}'.format(np.round(clf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

"""After experiments with hidden layers, conclusion arise. Neural networks models do have high accuracy, but they struggle with classification of class2 and in some instances class3. We got the best results with following model: 
solver = 'adam', activation = 'relu', hidden layers = (n/16, n/64, n/128)

## Ensemble

Okay, the next  models are created with the ensemble methods, and we will use them to try to improve the results we got so far.
A lot of differents models will be tested.

### RandomForests

We will first start with bagging method, **Random Forest Classifier**.
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

rfc = RandomForestClassifier(n_estimators=1000, criterion='gini')
rfc.fit(X_train, y_train)

y_pred = rfc.predict(X_test)

print('accuracy train: {}'.format(np.round(rfc.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

from joblib import dump
dump(clf, '/content/gdrive/My Drive/ip_files/models/randomForest_estimators1000_criterionGini.pkl')

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

rfc = RandomForestClassifier(n_estimators=1000, criterion='entropy')
rfc.fit(X_train, y_train)

y_pred = rfc.predict(X_test)

print('accuracy train: {}'.format(np.round(rfc.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

"""We got higher accuracy for gini, but lower for entropy. Both models cannot classify class2 or class3 correctly.
Now, let's test balanced class weights
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

rfc = RandomForestClassifier(n_estimators=1000, criterion='gini', class_weight='balanced')
rfc.fit(X_train, y_train)

y_pred = rfc.predict(X_test)

print('accuracy train: {}'.format(np.round(rfc.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

from joblib import dump
dump(clf, '/content/gdrive/My Drive/ip_files/models/randomForest_estimators1000_criterionGini_weightsBalanced.pkl')

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

rfc = RandomForestClassifier(n_estimators=1000, criterion='entropy', class_weight='balanced')
rfc.fit(X_train, y_train)

y_pred = rfc.predict(X_test)

print('accuracy train: {}'.format(np.round(rfc.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

"""There are no improvements in results. Classification of classes 2 and 3 is still a problem. Let;s experiment with parameters a little bit."""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

rfc = RandomForestClassifier(n_estimators=1000, criterion='gini', min_samples_split=3)
rfc.fit(X_train, y_train)

y_pred = rfc.predict(X_test)

print('accuracy train: {}'.format(np.round(rfc.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

rfc = RandomForestClassifier(n_estimators=1000, criterion='gini', min_samples_split=3, min_samples_leaf=3)
rfc.fit(X_train, y_train)

y_pred = rfc.predict(X_test)

print('accuracy train: {}'.format(np.round(rfc.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

"""No significant improvements.

### Gradient Boosting

Loss can be deviance or exponential. Exponental can only be used for 2 classes (we have seven here), so we will only try deviance one.
"""

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

gbc = GradientBoostingClassifier(loss='deviance', n_estimators=2)
gbc.fit(X_train, y_train)

y_pred = gbc.predict(X_test)

print('accuracy train: {}'.format(np.round(gbc.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

gbc = GradientBoostingClassifier(loss='deviance', n_estimators=100)
gbc.fit(X_train, y_train)

y_pred = gbc.predict(X_test)

print('accuracy train: {}'.format(np.round(gbc.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

gbc = GradientBoostingClassifier(loss='deviance', n_estimators=700)
gbc.fit(X_train, y_train)

y_pred = gbc.predict(X_test)

print('accuracy train: {}'.format(np.round(gbc.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

from joblib import dump
dump(gbc, '/content/gdrive/My Drive/ip_files/models/gradientBoost_estimators700_lossDeviance.pkl')

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

gbc = GradientBoostingClassifier(loss='deviance', n_estimators=1000)
gbc.fit(X_train, y_train)

y_pred = gbc.predict(X_test)

print('accuracy train: {}'.format(np.round(gbc.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

"""Results are good. We got better results for higher number of estimators, witch was expected... This model cannot classify class2 correctly, but it might do better for even higher number of estimators. Unfortunatly, we don't have enough time on Colab to test that.

### Bagging

Now we will try to upgrade our accuracy with bagging on some models that have already gave us good results, like **KNN** with seven neighbors and **SVM** with polynomial kernel with one degree. Also **NN** will be tested.

We got pretty much the same results for weighted and non-weightd method in **KNN** before. The weighted result is better, so we will use that model.
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

knn = KNeighborsClassifier(n_neighbors=6, weights='distance')

bclf = BaggingClassifier(base_estimator=knn, n_estimators=4, max_samples=0.2)
bclf.fit(X_train, y_train)

y_pred = bclf.predict(X_test)

print('accuracy train: {}'.format(np.round(bclf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

knn = KNeighborsClassifier(n_neighbors=6, weights='distance')

bclf = BaggingClassifier(base_estimator=knn, n_estimators=4, max_samples=0.25)
bclf.fit(X_train, y_train)

y_pred = bclf.predict(X_test)

print('accuracy train: {}'.format(np.round(bclf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

from joblib import dump

dump(bclf, '/content/gdrive/My Drive/ip_files/models/bagging_KNN6distance_estimators4_maxSamples25pct.pkl')

"""Prameters are, n_estimators = **4** and max_samples = **0.25**. Unfortunately,  for greater values I got MemoryError, A.K.A. not enough RAM, so if I want to increase one of them, I also have to decrease the other. This means that our flexibility very is limited.

No improvements with bagging, but we cannot test this for higher values of parameters...

Now we will do the same thing for SVC
"""

from sklearn.svm import SVC
from sklearn.ensemble import BaggingClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

svm = SVC(C=300, kernel='poly', gamma='scale', degree=1, class_weight=None)

bclf = BaggingClassifier(base_estimator=svm, n_estimators=10, max_samples=0.6)
bclf.fit(X_train, y_train)

y_pred = bclf.predict(X_test)

print('accuracy train: {}'.format(np.round(bclf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

from sklearn.svm import SVC
from sklearn.ensemble import BaggingClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

svm = SVC(C=300, kernel='poly', gamma='scale', degree=1, class_weight=None)

bclf = BaggingClassifier(base_estimator=svm, n_estimators=10, max_samples=1.0)
bclf.fit(X_train, y_train)

y_pred = bclf.predict(X_test)

print('accuracy train: {}'.format(np.round(bclf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

from joblib import dump

dump(bclf, '/content/gdrive/My Drive/ip_files/models/bagging_svm300PolyScale_estimators10_maxSamples100pct.pkl')

"""The hardware limitations are still present, but for this model we could put greater values for parameters than in the knn one.
 This model is good, but one we got without bagging is better.

Bagging of neural networks
"""

from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

n = len(X_train.loc[0, :])
clf = MLPClassifier(solver='adam', activation='relu', hidden_layer_sizes=(n // 16, n // 64, n // 128))

bclf = BaggingClassifier(base_estimator=clf, n_estimators=3)
bclf.fit(X_train, y_train)

y_pred = bclf.predict(X_test)

print('accuracy train: {}'.format(np.round(bclf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

from joblib import dump

dump(bclf, '/content/gdrive/My Drive/ip_files/models/bagging_nnAdamRelu_estimators3_maxSamples100pct.pkl')

"""Memory would allow us to have more estimators than 3, but the Google is giving us runtime of 12 hours and we cannot execute this code in that time

### Boosting

Okay, with bagging finished, we can focus on boosting. We will use **AdaBoost** for our classification.
**Knn** does not support sample weights, and therefore we cannot apply AdaBoost to it. The same thing apply for MLPClassifier as well.

We will start with **Decision trees**. Entropy gave us better results without boosting, so we will use entropy models for boosting. We will boost two models, one with class weights and one without
"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

dtc = DecisionTreeClassifier(criterion='entropy')

clf = AdaBoostClassifier(base_estimator=dtc, n_estimators=10)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

print('accuracy train: {}'.format(np.round(clf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

dtc = DecisionTreeClassifier(criterion='entropy')

clf = AdaBoostClassifier(base_estimator=dtc, n_estimators=1000)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

print('accuracy train: {}'.format(np.round(clf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

"""Result is better when we boost with less estimators, but we menaged to classify some of the patterns of class2 correctly.

Now balanced class weights.
"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

dtc = DecisionTreeClassifier(criterion='entropy', class_weight='balanced')

clf = AdaBoostClassifier(base_estimator=dtc, n_estimators=10)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

print('accuracy train: {}'.format(np.round(clf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

dtc = DecisionTreeClassifier(criterion='entropy', class_weight='balanced')

clf = AdaBoostClassifier(base_estimator=dtc, n_estimators=1000)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

print('accuracy train: {}'.format(np.round(clf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

"""No improvements.

**Random forests** are next. We will use gini as a criterion here, for the same reason we used entropy in Decision trees.
Two models, one with class weights and one without.
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

rfc = RandomForestClassifier(n_estimators=1000, criterion='gini')

clf = AdaBoostClassifier(base_estimator=rfc, n_estimators=500)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

print('accuracy train: {}'.format(np.round(clf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

rfc = RandomForestClassifier(n_estimators=1000, criterion='gini', class_weight='balanced')

clf = AdaBoostClassifier(base_estimator=rfc, n_estimators=10)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

print('accuracy train: {}'.format(np.round(clf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

"""The values here are pretty much the same as the ones we got without boosting.

We will boost svm now. To allow boosting with AdaBoost on svc, we need to ser probability parameter to 'True' value.
"""

from sklearn.svm import SVC
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

svm = SVC((C=300, kernel='poly', gamma='scale', degree=1, class_weight=None, probability=True)

clf = AdaBoostClassifier(base_estimator=svm, n_estimators=2)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

print('accuracy train: {}'.format(np.round(clf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

"""Can't execute this code in Colab machine because of the **Runtime** (12 hours).

### Voting classifiers

And now, our last approach, **Voting classifiers**. We will create some models from before, and do the voting.
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.ensemble import VotingClassifier

from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

rfc = RandomForestClassifier(n_estimators=1000, criterion='gini')
knn = KNeighborsClassifier(n_neighbors=6, weights='distance')
mnb = MultinomialNB()
svc = SVC(C=300, kernel='poly', gamma='scale', degree=1, class_weight=None)

vclf = VotingClassifier(estimators=[('RFC', rfc), ('KNN', knn), ('MNB', mnb), ('SVM', svc)], voting='hard')
vclf.fit(X_train, y_train)

y_pred = vclf.predict(X_test)

print('accuracy train: {}'.format(np.round(vclf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

"""Results are fine, but class2 is not classified correctly.

The same thing, with same models used for voting, but with weights for each of the models added. Weights have the value they have because the better models from before should have greater weights. Naive Bayes one is not that good so it will have lower waight value. We will do the oppesite for our best model SVM, setting it's weight to highest value of all.
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.ensemble import VotingClassifier
lords
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

rfc = RandomForestClassifier(n_estimators=1000, criterion='gini')
knn = KNeighborsClassifier(n_neighbors=6, weights='distance')
mnb = MultinomialNB()
svm = SVC(C=300, kernel='poly', gamma='scale', degree=1, class_weight=None)

vclf = VotingClassifier(estimators=[('RFC', rfc), ('KNN', knn), ('MNB', mnb), ('SVM', svm)], voting='hard', weights=[1.5, 1.7, 1, 2.5])
vclf.fit(X_train, y_train)

y_pred = vclf.predict(X_test)

print('accuracy train: {}'.format(np.round(vclf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

"""We did imporove the result with weights added, but we mijust might imporve accuracy more by changing classifiers.

Now we will do the voting, but the models used will be just SVC with different parameters.
"""

from sklearn.svm import SVC
from sklearn.ensemble import VotingClassifier

from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

svc1 = SVC(C=100, kernel='poly', degree=1, gamma='scale')
svc2 = SVC(C=300, kernel='poly', degree=1, gamma='scale')
svc3 = SVC(C=100, kernel='rbf', gamma='scale')
svc4 = SVC(C=300, kernel='rbf', gamma='scale')

vclf = VotingClassifier(estimators=[('SVC1', svc1), ('SVC2', svc2), ('SVC3', svc3), ('SVC4', svc4)], voting='hard', weights=[1.1, 1, 1, 1.1])
vclf.fit(X_train, y_train)

y_pred = vclf.predict(X_test)

print('accuracy train: {}'.format(np.round(vclf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

from joblib import dump

dump(vclf, '/content/gdrive/My Drive/ip_files/models/voting_svm_examples.pkl')

from sklearn.svm import SVC
from sklearn.ensemble import VotingClassifier

from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

svc1 = SVC(C=100, kernel='poly', degree=1, gamma='scale')
svc2 = SVC(C=300, kernel='poly', degree=1, gamma='scale')
svc3 = SVC(C=100, kernel='rbf', gamma='scale')
svc4 = SVC(C=300, kernel='rbf', gamma='scale')

vclf = VotingClassifier(estimators=[('SVC1', svc1), ('SVC2', svc2), ('SVC3', svc3), ('SVC4', svc4)], voting='hard', weights=[1.5, 1, 1, 1.5])
vclf.fit(X_train, y_train)

y_pred = vclf.predict(X_test)

print('accuracy train: {}'.format(np.round(vclf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

"""Both of the models gave us same results, regardless of the model weights. Results are **very good!**

Now, we will create classifier that can classify class2 and class3 hopefully good, and two of the others, and build voting model. <--- TODO
"""

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, f1l_score, recall_score
import numpy as np


weights = {
    'class1' : 0.0,
    'class2' : 100,
    'class3' : 30,
    'class4' : 0.0,
    'class5' : 0.0,
    'class6' : 0.0,
    'class7' : 0.0,
}

dtc = SVC(C=300, kernel='rbf', gamma='scale', class_weight=weights)
dtc.fit(X_train, y_train)

y_pred = dtc.predict(X_test)

print('accuracy train: {}'.format(np.round(dtc.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

from sklearn.svm import SVC
from sklearn.ensemble import VotingClassifier

from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

weights = {
    'class1' : 1,
    'class2' : 1000,
    'class3' : 300,
    'class4' : 1,
    'class5' : 1,
    'class6' : 1,
    'class7' : 1,
}

svc1 = SVC(C=100, kernel='poly', degree=1, gamma='scale')
svc2 = SVC(C=300, kernel='poly', degree=1, gamma='scale')
svc3 = SVC(C=300, kernel='rbf', gamma='scale', class_weight=weights) # this will classify classes 2 and 3 okayish hopefully
svc4 = SVC(C=300, kernel='rbf', gamma='scale')

vclf = VotingClassifier(estimators=[('SVC1', svc1), ('SVC2', svc2), ('SVC3', svc3), ('SVC4', svc4)], voting='hard', weights=[1.1, 1, 1.2, 1.1])
vclf.fit(X_train, y_train)

y_pred = vclf.predict(X_test)

print('accuracy train: {}'.format(np.round(vclf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

"""This won't work for some reason... Will look into it later!"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC

from sklearn.ensemble import VotingClassifier

rfc = RandomForestClassifier(n_estimators=1000, criterion='gini')
dt = DecisionTreeClassifier(criterion='entropy')
mnb = MultinomialNB()
svc = SVC(C=100, kernel='poly', degree=1, gamma='scale', probability=True)

vclf = VotingClassifier(estimators=[('RFC', rfc), ('DT', dt), ('MNB', mnb), ('SVM', svc)], voting='soft')
vclf.fit(X_train, y_train)

import numpy as np

# random petterns from test set
print('pattern1')
print(np.round(vclf.predict_proba([X_test.iloc[4444]]), 3))

print('pattern2')
print(np.round(vclf.predict_proba([X_test.iloc[800]]), 3))

print('pattern3')
print(np.round(vclf.predict_proba([X_test.iloc[300]]), 3))

print('pattern4')
print(np.round(vclf.predict_proba([X_test.iloc[2222]]), 3))

print('pattern5')
print(np.round(vclf.predict_proba([X_test.iloc[1471]]), 3))

print('pattern6')
print(np.round(vclf.predict_proba([X_test.iloc[0]]), 3))

"""This is showing us the probability that the selected pattern belongs to some of the classes.
Pattern4 is probably class6 because probability is highest. 

Let's check if the classification is correct.
"""

print('pattern1')
print(y_test.iloc[4444])

print('pattern2')
print(y_test.iloc[800])

print('pattern3')
print(y_test.iloc[300])

print('pattern4')
print(y_test.iloc[2222])

print('pattern5')
print(y_test.iloc[1471])

print('pattern6')
print(y_test.iloc[0])

"""Well... kinda. Note that pattern6 belongs to class7 (second lowest value) but our model classified it to class1.
These are the results:
"""

from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import numpy as np

y_pred = vclf.predict(X_test)

print('accuracy train: {}'.format(np.round(vclf.score(X_train, y_train), 4)))
print('accuracy test: {}'.format(np.round(accuracy_score(y_test, y_pred), 4)))
print('recall score: {}'.format(np.round(recall_score(y_test, y_pred, average=None), 4)))
print('f1_score: {}'.format(np.round(f1_score(y_test, y_pred, average=None), 4)))
print('confusion matrix: \n{}'.format(confusion_matrix(y_test, y_pred)))

"""They are overall very good."""
